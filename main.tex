\documentclass[8pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{enumitem}
\geometry{margin=0.5in}

\definecolor{defcolor}{RGB}{0,102,204}
\definecolor{thmcolor}{RGB}{204,0,102}
\definecolor{propcolor}{RGB}{255,102,0}
\definecolor{remcolor}{RGB}{0,153,51}

\newcommand{\mydef}[1]{\textcolor{defcolor}{\textbf{#1}}}
\newcommand{\mythm}[1]{\textcolor{thmcolor}{\textbf{#1}}}
\newcommand{\myprop}[1]{\textcolor{propcolor}{\textbf{#1}}}
\newcommand{\myrem}[1]{\textcolor{remcolor}{\textbf{#1}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Prob}{\mathbb{P}}

\setlength{\columnsep}{0.3in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{3pt}

\begin{document}

\title{\textbf{Statistique Inférentielle - Résumé Complet}}
\date{}
\maketitle

\section*{\mydef{I. ÉCHANTILLONNAGE ET MODÉLISATION}}

\textbf{Échantillon aléatoire} : $(X_1,\ldots,X_n)$ v.a. i.i.d. de même loi que $X$

\textbf{Modèle statistique} : famille de lois $(P_\theta)_{\theta \in \Theta}$

\mydef{Modèle paramétrique}: $\Theta \subset \R^d$, identifiable si $\theta \mapsto P_\theta$ injective.

\mydef{$n$-échantillon}: $X=(X_1,\dots,X_n)$ i.i.d. de loi $p_\theta$. Modèle: $(\Omega=\Psi^n, \mathcal{A}=\mathcal{B}^{\otimes n}, P_\theta=(p_\theta)^{\otimes n})$.

\textbf{Démarche statistique} :
\begin{enumerate}[nosep]
\item \mydef{Modélisation} : $X \sim P_\theta$ avec $\theta$ inconnu
\item \mydef{Inférence} : estimer $\theta$ à partir des observations
\end{enumerate}

\section*{\mydef{II. MODES DE CONVERGENCE}}

\textbf{Convergence en probabilité} : $X_n \xrightarrow{P} X$
$$\forall \varepsilon > 0, \quad P(|X_n - X| \geq \varepsilon) \xrightarrow{n \to \infty} 0$$

\textbf{Convergence presque sûre} : $X_n \xrightarrow{p.s.} X$
$$P\left(\left\{\omega : \lim_{n \to \infty} X_n(\omega) = X(\omega)\right\}\right) = 1$$

\textbf{Convergence en loi} : $X_n \xrightarrow{\mathcal{L}} X$
$$\forall \varphi \text{ continue bornée}, \quad E[\varphi(X_n)] \to E[\varphi(X)]$$

\textbf{Convergence $L^2$} : $X_n \xrightarrow{\|\cdot\|_2} X$
$$E[|X_n - X|^2] \to 0$$

\subsection{\myprop{Hiérarchie des convergences}}
$$\text{p.s.} \Rightarrow \text{proba} \Rightarrow \text{loi}$$
$$L^2 \Rightarrow \text{proba}$$

\myrem{Réciproque fausse sauf si limite = constante}

\subsection{\myprop{Critères pour convergence en loi}}
\begin{itemize}[nosep]
\item \textbf{Paul Lévy} : $\Phi_{X_n}(t) \to \Phi_X(t)$ $\forall t$
\item \textbf{Fonctions répartition} : $F_{X_n}(x) \to F_X(x)$ aux points de continuité
\end{itemize}

\section*{\mydef{III. INÉGALITÉS ET THÉORÈMES ASYMPTOTIQUES}}

\subsection{\mythm{Inégalité de Markov}}
$$P(|X| \geq c) \leq \frac{E[|X|^p]}{c^p}$$

\subsection{\mythm{Inégalité de Bienaymé-Tchebychev}}
$$P(|X - E[X]| \geq c) \leq \frac{\text{Var}(X)}{c^2}$$

\subsection{\mythm{Inégalité de Hoeffding}}
$X_1,\ldots,X_n$ indép., $a_i \leq X_i \leq b_i$, $S_n = \sum X_i$
$$P(|S_n - E[S_n]| \geq c) \leq 2\exp\left(-\frac{2c^2}{\sum_{i=1}^n (b_i-a_i)^2}\right)$$

\myrem{Hoeffding : décroissance exponentielle vs polynomiale (Markov)}

\textbf{Application Borel-Cantelli} : Si $\sum P(|X_n - X| \geq \varepsilon) < \infty$ alors $X_n \xrightarrow{p.s.} X$

\subsection{\mythm{Loi des Grands Nombres}}
$X_1,\ldots,X_n$ i.i.d., $E[X_1] = m < \infty$
$$\frac{S_n}{n} \xrightarrow{p.s.} m$$

\subsection{\mythm{Théorème Central Limite}}
$X_1,\ldots,X_n$ i.i.d., $\text{Var}(X_1) = \sigma^2 < \infty$
$$\sqrt{n}\left(\frac{S_n}{n} - m\right) \xrightarrow{\mathcal{L}} \mathcal{N}(0,\sigma^2)$$

\myrem{TCL : base des intervalles de confiance asymptotiques}

\section*{\mydef{IV. OPÉRATIONS SUR LES LIMITES}}

\subsection{\mythm{Théorème de continuité}}
Si $X_n \to X$ et $P(X \in D_g) = 0$ ($D_g$ = points de discontinuité)
$$g(X_n) \text{ hérite du mode de convergence}$$

\subsection{\mythm{Théorème de Slutsky}}
$X_n \xrightarrow{\mathcal{L}} X$, $Y_n \xrightarrow{P} a$ (constante)
$$X_n + Y_n \xrightarrow{\mathcal{L}} X + a$$
$$X_n Y_n \xrightarrow{\mathcal{L}} aX$$

\subsection{\mythm{Delta méthode}}
$v_n(X_n - a) \xrightarrow{\mathcal{L}} X$, $g$ dérivable en $a$
$$v_n(g(X_n) - g(a)) \xrightarrow{\mathcal{L}} g'(a)X$$

\myrem{Utilité : propager incertitude par transformations}

\section*{\mydef{V. VECTEURS GAUSSIENS}}

\subsection{Définition}
$X = (X_1,\ldots,X_d)'$ gaussien si $\forall u \in \mathbb{R}^d$, $u'X \sim \mathcal{N}$

\subsection{\myprop{Propriétés fondamentales}}
\begin{itemize}[nosep]
\item Caractérisé par $\mu = E[X]$ et $\Sigma = \text{Var}(X)$
\item Notation : $X \sim \mathcal{N}_d(\mu, \Sigma)$
\item Fonction caractéristique : 
$$\Phi(t) = \exp(it'\mu - \frac{1}{2}t'\Sigma t)$$
\end{itemize}

\subsection{\myprop{Propriétés importantes}}
\begin{itemize}[nosep]
\item \textbf{Indépendance} : composantes indép. $\Leftrightarrow$ $\Sigma$ diagonale
\item \textbf{Transformation linéaire} : $A + BX \sim \mathcal{N}_q(A + B\mu, B\Sigma B')$
\item \textbf{Densité} (si $\det \Sigma \neq 0$) :
$$f_X(x) = \frac{1}{\sqrt{(2\pi)^d \det \Sigma}} \exp\left(-\frac{1}{2}(x-\mu)'\Sigma^{-1}(x-\mu)\right)$$
\end{itemize}

\subsection{Loi du $\chi^2$}
$X_1,\ldots,X_k$ i.i.d. $\mathcal{N}(\mu_i, 1)$
$$\|X\|^2 \sim \chi^2(k, \|\mu\|^2)$$
Si $\mu = 0$ : $\|X\|^2 \sim \chi^2(k)$

\subsection{\mythm{Théorème de Cochran}}
$X \sim \mathcal{N}_d(\mu, I_d)$, $E_1,\ldots,E_p$ sous-espaces orthogonaux
\begin{itemize}[nosep]
\item $\Pi_{E_i}(X)$ gaussiens indépendants
\item $\|\Pi_{E_i}(X)\|^2 \sim \chi^2(r_i, \|\Pi_{E_i}\mu\|^2)$
\end{itemize}

\myrem{Application : construction de statistiques en cadre gaussien}

\section*{\mydef{VI. ESTIMATION}}

\mydef{Estimateur}: Statistique $T=h(X)$ pour estimer $g(\theta)$.

\mydef{Biais}: $b_\theta(\hat{g}) = \|g(\theta) - \E_\theta\hat{g}\|$. Sans biais si $\E_\theta\hat{g}=g(\theta)$.

\mydef{Risque quadratique}: $R_\theta(\hat{g}) = \E_\theta[\|\hat{g}-g(\theta)\|^2]$.

\myprop{Décomposition biais-variance}: 
$$R_\theta(\hat{g}) = \Var(\hat{g}) + b_\theta(\hat{g})^2$$

\mydef{Convergence}:
\begin{itemize}[nosep]
\item Asympt. sans biais: $\E_\theta\hat{g}_n \to g(\theta)$
\item Convergence en moyenne quad.: $R_\theta(\hat{g}_n) \to 0$
\item Consistant: $\hat{g}_n \xrightarrow{P_\theta} g(\theta)$
\item Fortement consistant: $\hat{g}_n \xrightarrow{p.s.} g(\theta)$
\end{itemize}

\mydef{Vitesse}: $v_n(\hat{g}_n - g(\theta)) \xrightarrow{\mathcal{L}} \ell(\theta)$ non dégénérée. Si $\ell = \mathcal{N}$ et $v_n=\sqrt{n}$: asympt. normal.

\subsection{Méthode des Moments}
Estimer $\phi(\theta)=\E_\theta[h(X_1)]$ par $\hat{\phi}=\frac{1}{n}\sum h(X_i)$. Inverser si possible.

\subsection{Maximum de Vraisemblance}
\mydef{Fonction de vraisemblance}: $L_n(\theta)=\prod_{i=1}^n s_\theta(X_i)$, $\ell_n(\theta)=\ln L_n(\theta)$.

\mydef{EMV}: $\hat{\theta} \in \arg\max_{\theta\in\Theta} L_n(\theta)$.

\mydef{Score}: $\ell_n'(X,\theta)=\frac{\partial}{\partial\theta}\ln L_n(X,\theta)$.

\mydef{Information de Fisher}: 
$$I_n(\theta)=\E_\theta[(\ell_n'(X,\theta))^2] = -\E_\theta[\ell_n''(X,\theta)]$$

\myprop{Efficacité asymptotique}: Sous régularité, si $\sqrt{n}(\hat{\theta}-\theta) \xrightarrow{\mathcal{L}} \mathcal{N}(0,\sigma^2(\theta))$, borne inf: $\sigma^2(\theta) \geq 1/I_1(\theta)$. EMV asympt. efficace si égalité.

\section*{\mydef{VII. ESTIMATION GAUSSIENNE}}

Modèle: $X_1,\dots,X_n \sim \mathcal{N}(\mu,\sigma^2)$.

\mydef{Estimateurs}:
$$\bar{X}_n=\frac{1}{n}\sum X_i,\quad S_n^2=\frac{1}{n-1}\sum(X_i-\bar{X}_n)^2$$

\mythm{Fisher-Cochran}:
\begin{enumerate}[nosep]
\item $\bar{X}_n \sim \mathcal{N}(\mu, \sigma^2/n)$
\item $S_n^2 \sim \frac{\sigma^2}{n-1}\chi^2(n-1)$
\item $\bar{X}_n$ et $S_n^2$ indépendantes
\item $\sqrt{n}\frac{\bar{X}_n-\mu}{S_n} \sim \tau(n-1)$
\end{enumerate}

\section*{\mydef{VIII. INTERVALLES DE CONFIANCE}}

\mydef{IC de niveau $1-\alpha$}: $\hat{C}$ tel que $P_\theta(\theta\in\hat{C}) \geq 1-\alpha$ $\forall\theta$.

\subsection{Cas Gaussien}
\myprop{IC pour $\mu$} ($\sigma$ inconnu):
$$\left[\bar{X}_n \pm \frac{S_n t_{n-1,1-\alpha/2}}{\sqrt{n}}\right]$$
où $t_{n-1,1-\alpha/2}$: quantile Student.

\myprop{IC pour $\sigma^2$}:
$$\left[\frac{(n-1)S_n^2}{\chi^2_{n-1,1-\alpha/2}}, \frac{(n-1)S_n^2}{\chi^2_{n-1,\alpha/2}}\right]$$

\myprop{IC asymptotique} pour $\mu$ (var. finie):
$$\left[\bar{X}_n \pm z_{1-\alpha/2}\sqrt{\frac{\hat{\sigma}^2_n}{n}}\right]$$

\myrem{Réalisation}: L'IC est aléatoire, sa réalisation est fixe.

\section*{\mydef{IX. TESTS D'HYPOTHÈSES}}

\mydef{Test}: $\Phi:X \mapsto \{0,1\}$, $\Phi=1$ = rejet $H_0$.

\mydef{Hypothèses}: $H_0:\theta\in\Theta_0$ vs $H_1:\theta\in\Theta_1$.

\mydef{Erreurs}:
\begin{itemize}[nosep]
\item 1ère espèce: $e_{pr}=\sup_{\theta\in\Theta_0} P_\theta(\text{rejeter }H_0)$
\item 2ème espèce: $e_{sc}=\sup_{\theta\in\Theta_1} P_\theta(\text{accepter }H_0)$
\end{itemize}

\mydef{Niveau}: Test de niveau $\alpha$ si $e_{pr} \leq \alpha$.

\mydef{Puissance}: $\pi(\theta)=P_\theta(\text{rejeter }H_0)$.

\mydef{Uniformément plus puissant}: $\pi(\theta) \geq \pi'(\theta)$ $\forall\theta\in\Theta_1$.

\subsection{Tests Gaussiens}
\myprop{Test de Student} ($\mu$):
$$T_n=\sqrt{n}\frac{\bar{X}_n-\mu_0}{S_n} \sim \tau(n-1) \text{ sous }H_0$$
Rejet si $|T_n| > t_{n-1,1-\alpha/2}$ (bilatère) ou $T_n > t_{n-1,1-\alpha}$ (unilatère).

\myprop{Test variance}:
$$\frac{(n-1)S_n^2}{\sigma_0^2} \sim \chi^2(n-1) \text{ sous }H_0$$

\myprop{Test 2 échantillons}:
\begin{itemize}[nosep]
\item Comparaison variances: $F=S_X^2/S_Y^2 \sim \mathcal{F}(n_x-1,n_y-1)$ sous $H_0$
\item Comparaison moyennes ($\sigma_x=\sigma_y$): 
$$T=\frac{\bar{X}-\bar{Y}}{S_p\sqrt{1/n_x+1/n_y}} \sim \tau(n_x+n_y-2)$$
\end{itemize}

\myprop{Test proportion}: 
$$U_n=\sqrt{n}\frac{\hat{p}-p_0}{\sqrt{p_0(1-p_0)}} \xrightarrow{\mathcal{L}} \mathcal{N}(0,1)$$

\myprop{Test de Wald}: Si $\sqrt{n}(\hat{\theta}-\theta)/S \xrightarrow{\mathcal{L}} \mathcal{N}(0,1)$, rejet si $|W|>z_{1-\alpha/2}$.

\subsection{p-value}
\mydef{p-value}: Plus petit $\alpha$ pour lequel on rejette $H_0$.

\myrem{Interprétation}:
\begin{itemize}[nosep]
\item p-value $< \alpha$: rejet $H_0$
\item p-value grande: pas de preuve contre $H_0$
\item Résultat significatif si p-value très faible
\end{itemize}

\section*{\mydef{X. RÉGRESSION LINÉAIRE}}

\subsection{Modèle Linéaire}
\mydef{Modèle de régression simple}:
$$Y_i = ax_i + b + \varepsilon_i,\quad i=1,\ldots,n$$
avec $\varepsilon_i$ i.i.d., $\E\varepsilon_i=0$, $\Var\varepsilon_i=\sigma^2$.

\mydef{Modèle de régression multiple}:
$$\mathbf{Y} = \mathbf{X}\boldsymbol{\theta} + \boldsymbol{\varepsilon}$$
où $\mathbf{X}_{n\times p}$ matrice du design, $\boldsymbol{\theta}\in\R^p$.

\mydef{Hypothèses du Modèle}:
\begin{itemize}[nosep]
\item A1: $\E\boldsymbol{\varepsilon} = 0$ (erreurs centrées)
\item A2: $\Var\varepsilon_i = \sigma^2$ (homoscédasticité)
\item A3: $\varepsilon_i$ indépendantes
\item A4: $\varepsilon_i \sim \mathcal{N}(0,\sigma^2)$ (normalité)
\end{itemize}

\mydef{Modèle régulier}: $\mathbf{X}$ injective ($rg(\mathbf{X})=p$).\\
\mydef{Modèle singulier}: $\mathbf{X}$ non injective.

\subsection{Estimation par Moindres Carrés}
\mydef{Estimateur MC}: Minimise $\|\mathbf{Y} - \mathbf{X}\boldsymbol{\theta}\|^2$

\myprop{Équations normales}:
$$\mathbf{X}'\mathbf{X}\boldsymbol{\theta} = \mathbf{X}'\mathbf{Y}$$

\myprop{Cas régulier}:
$$\hat{\boldsymbol{\theta}}_{MC} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}$$

\myprop{Propriétés}:
\begin{itemize}[nosep]
\item Sans biais: $\E\hat{\boldsymbol{\theta}} = \boldsymbol{\theta}$
\item Variance: $\Var\hat{\boldsymbol{\theta}} = \sigma^2(\mathbf{X}'\mathbf{X})^{-1}$
\item Optimal (Gauss-Markov): Variance minimale parmi estimateurs linéaires sans biais
\end{itemize}

\subsection{Valeurs Prédites et Résidus}
\mydef{Valeurs prédites}: $\hat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\theta}} = P_{\text{Im}(\mathbf{X})}\mathbf{Y}$

\mydef{Résidus}: $\hat{\boldsymbol{\varepsilon}} = \mathbf{Y} - \hat{\mathbf{Y}} = P_{\text{Im}(\mathbf{X})^\perp}\mathbf{Y}$

\mydef{Somme des carrés résiduels}:
$$SC_{res} = \|\hat{\boldsymbol{\varepsilon}}\|^2 = \|\mathbf{Y} - \hat{\mathbf{Y}}\|^2$$

\mydef{Estimateur de $\sigma^2$}:
$$S^2 = \frac{SC_{res}}{n-p} \quad (\text{sans biais})$$

\subsection{Estimateur du Maximum de Vraisemblance}
\myprop{Sous A1-A4 (normalité)}:
\begin{itemize}[nosep]
\item $\hat{\boldsymbol{\theta}}_{MV} = \hat{\boldsymbol{\theta}}_{MC}$
\item $\hat{\sigma}^2_{MV} = \frac{SC_{res}}{n}$ (biaisé)
\end{itemize}

\subsection{Théorème de Fisher-Cochran (Régression)}
\mythm{Sous A1-A4}:
\begin{enumerate}[nosep]
\item $\hat{\mathbf{Y}} \sim \mathcal{N}_n(\mathbf{X}\boldsymbol{\theta}, \sigma^2 P_{\text{Im}(\mathbf{X})})$
\item $\frac{SC_{res}}{\sigma^2} \sim \chi^2(n-p)$
\item $\hat{\mathbf{Y}}$ et $S^2$ indépendants
\item Si modèle régulier: $\hat{\boldsymbol{\theta}} \sim \mathcal{N}_p(\boldsymbol{\theta}, \sigma^2(\mathbf{X}'\mathbf{X})^{-1})$
\end{enumerate}

\subsection{Régression Simple}
\myprop{Estimateurs}:
$$\hat{a} = \frac{\Cov(x,Y)}{\Var(x)},\quad \hat{b} = \bar{Y} - \hat{a}\bar{x}$$

\myprop{Matrice de covariance}:
$$(\mathbf{X}'\mathbf{X})^{-1} = \frac{1}{n\Var(x)}\begin{bmatrix}
\bar{x}^2 + \Var(x) & -\bar{x} \\
-\bar{x} & 1
\end{bmatrix}$$

\subsection{Intervalles de Confiance}
\myprop{Pour $\theta_j$}:
$$\hat{\theta}_j \pm S\sqrt{w_{jj}}\,t_{n-p,1-\alpha/2}$$
où $w_{jj} = [(\mathbf{X}'\mathbf{X})^{-1}]_{jj}$

\myprop{Pour $\E(Y_x)$ (nouvelle observation)}:
$$\hat{Y}_x \pm S\sqrt{\mathbf{x}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x}}\,t_{n-p,1-\alpha/2}$$

\myprop{Intervalle de prédiction}:
$$\hat{Y}_x \pm S\sqrt{1 + \mathbf{x}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x}}\,t_{n-p,1-\alpha/2}$$

\subsection{Tests d'Hypothèses}
\myprop{Test de Student pour $\theta_j$}:
$$T_j = \frac{\hat{\theta}_j}{S\sqrt{w_{jj}}} \sim \tau(n-p) \text{ sous } H_0:\theta_j=0$$

Règle de décision: Rejet si $|T_j| > t_{n-p,1-\alpha/2}$

\myprop{F-test pour modèles emboîtés}:
\mydef{Sous-modèle}: $\text{Im}(\mathbf{X}_\omega) \subset \text{Im}(\mathbf{X}_\Omega)$, $rg(\mathbf{X}_\omega)=q<p$

$$F_{\omega|\Omega} = \frac{(SC_\omega - SC_\Omega)/(p-q)}{SC_\Omega/(n-p)} \sim \mathcal{F}(p-q,n-p) \text{ sous } H_0$$

Règle de décision: Rejet si $F_{\omega|\Omega} > f_{p-q,n-p,1-\alpha}$

\subsection{Régions de Confiance}
\myprop{Ellipsoïde de confiance}:
$$\{\mathbf{u} \in \R^p : (\hat{\boldsymbol{\theta}} - \mathbf{u})'\mathbf{X}'\mathbf{X}(\hat{\boldsymbol{\theta}} - \mathbf{u}) \leq pS^2 f_{p,n-p,1-\alpha}\}$$

\myprop{Méthode de Bonferroni}: 
$$\hat{I}_0 \times \cdots \times \hat{I}_{p-1} \text{ avec } 1-\alpha/p \text{ par paramètre}$$

\subsection{Robustesse}
\myprop{Sans hypothèse A4 (normalité)}:
Propriétés conservées:
\begin{itemize}[nosep]
\item $\hat{\boldsymbol{\theta}}_{MC}$ sans biais
\item $\Var\hat{\boldsymbol{\theta}}_{MC} = \sigma^2(\mathbf{X}'\mathbf{X})^{-1}$
\item Optimalité Gauss-Markov
\end{itemize}

Propriétés perdues:
\begin{itemize}[nosep]
\item Lois exactes ($\chi^2$, Student, Fisher)
\item Tests exacts de niveau $\alpha$
\end{itemize}

\myprop{Comportement asymptotique}:
Sous A1-A3, si $\frac{1}{n}\mathbf{X}'\mathbf{X} \to Q$ définie positive:
$$\sqrt{n}(\hat{\boldsymbol{\theta}}_n - \boldsymbol{\theta}) \xrightarrow{\mathcal{L}} \mathcal{N}_p(0,\sigma^2 Q^{-1})$$

Tests asymptotiques: F-test valide pour grands échantillons

\subsection{ANOVA à un facteur}
\mydef{Modèle}: $Y_{ij} = \mu_i + \varepsilon_{ij}$, $i=1,\ldots,m$, $j=1,\ldots,n_i$

\mydef{Écriture matricielle}: 
$$\mathbf{Y} = \mathbf{X}\boldsymbol{\mu} + \boldsymbol{\varepsilon}$$
avec $\mathbf{X}$ matrice indicatrices des groupes

\end{document}
