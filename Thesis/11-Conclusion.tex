\chapter{General Conclusion and Perspectives}
\label{chap:general_conclusion}
\addcontentsline{toc}{chapter}{General Conclusion and Perspectives}

\section*{General Conclusion}

This project looked at a major challenge in multi-agent reinforcement learning: how can a team of agents work together effectively when they can only see a small part of their environment? We saw that current methods, like the ${MA}^2E$ framework, use a random masking approach to help agents learn about the wider world. However, this random approach has a key weakness: it is disconnected from how well the agents are actually learning to play the game.

To fix this, we created and tested a new method called the \textbf{Learning-Informed Masking Framework ($LI-{MA}^2E$)}. The main idea is to use an agent's own performance during training to intelligently decide who to mask. We tested several different ways to score agents and found that a simple strategy based on the team's average TD-error worked best.

Our experiments showed that this idea is very effective. Compared to the baseline, our $LI-{MA}^2E$ method helped agents:
\begin{itemize}
    \item \textbf{Win more reliably and learn much faster}, especially on difficult maps where teamwork is critical.
    \item \textbf{Learn to win in a better, more efficient way}, achieving higher scores (mean test return) and more stable final strategies.
\end{itemize}
The reason for this success is that our method creates a more stable and focused learning process, as shown by the analysis of the TD-error. In short, our work proves that connecting the masking task directly to the agent's learning progress is a powerful way to improve teamwork in MARL.

\section*{Perspectives}

The results from this project also point to several exciting ideas for future research.

\begin{itemize}
    \item \textbf{Smarter, Adaptive Masking:} We noticed that our method's performance sometimes changed depending on the map. A great next step would be to create a system that can automatically learn and adapt its masking strategy for each new challenge it faces.

    \item \textbf{Exploring New Scoring Methods:} While TD-error worked very well, other signals could also be used to identify struggling agents. Future work could explore using metrics related to an agent's contribution to the team's success or even information from communication signals to create an even more powerful scoring function.

    \item \textbf{Real-World Applications:} The principles of $LI-{MA}^2E$ are general. It would be very valuable to test this framework outside of the StarCraft game environment, for example, with real-world applications like coordinating swarms of search-and-rescue drones or managing teams of robots in a warehouse.
\end{itemize}