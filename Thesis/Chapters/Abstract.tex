\documentclass[../Main.tex]{subfiles}
\begin{document}

% The proliferation of autonomous systems, from robotic swarms to distributed sensor networks, has underscored the need for intelligent and coordinated multi-agent behavior. Multi-Agent Reinforcement Learning (MARL) has emerged as a powerful paradigm for enabling agents to learn complex, collaborative strategies directly from interaction. By extending the principles of single-agent Reinforcement Learning (RL), MARL provides a framework{abstract}
% Cooperative Multi-Agent Reinforcement Learning (MARL) has shown great promise in solving complex coordination tasks, but leading algorithms like QMIX face significant scalability challenges. These challenges stem from centralized components that enforce global coordination among all agents at every timestep, leading to computational bottlenecks as the number of agents increases. This project investigates the hypothesis that such global coordination is often redundant and that the system is robust to information loss.

% To test this, we introduce and evaluate QMIX-Masked, a lightweight variant that applies random masking to agent inputs before they enter the central mixing network. Using the challenging StarCraft Multi-Agent Challenge (SMAC) as our primary benchmark, our empirical results demonstrate that system performance does not significantly degrade even with masking ratios as high as 60\%. This surprising robustness suggests that considerable redundancy exists in QMIX's global value decomposition. Our findings open promising avenues for developing more efficient and scalable selective coordination mechanisms, paving the way for the next generation of MARL algorithms.

The proliferation of autonomous systems—from robotic swarms to distributed sensor networks—has highlighted the need for intelligent and coordinated multi-agent behavior. Multi-Agent Reinforcement Learning (MARL) has emerged as a powerful paradigm for enabling agents to learn complex, collaborative strategies directly through interaction. By extending the principles of single-agent Reinforcement Learning (RL), MARL provides a framework for learning coordination in decentralized settings.

Among cooperative MARL algorithms, QMIX has shown great promise in solving complex coordination tasks. However, it suffers from scalability challenges due to its reliance on centralized components that enforce global coordination at every timestep. This centralized architecture leads to computational bottlenecks as the number of agents increases.

In this project, we investigate the hypothesis that such global coordination is often redundant and that MARL systems may be robust to partial information loss. To test this, we introduce QMIX-Masked, a lightweight variant of QMIX that applies random masking to agent inputs before they are processed by the central mixing network. Using the StarCraft Multi-Agent Challenge (SMAC) as our benchmark, we show that performance remains largely unaffected even with masking ratios as high as 60%.

These results reveal a surprising degree of redundancy in QMIX’s global value decomposition, suggesting that full agent observability may not be essential for effective coordination. Our findings open new directions for developing more scalable and efficient MARL algorithms that rely on selective coordination mechanisms.

\par\vspace*{\fill} % Moves keywords to the bottom of the page
\textbf{\textit{Keywords --}} Multi-Agent Reinforcement Learning (MARL), Deep Q-Learning, QMIX, Scalability, Value Decomposition, StarCraft Multi-Agent Challenge (SMAC). % Add all the keywords associated with your thesis here

\biblio % Needed for referencing to working when compiling individual subfiles - Do not remove
\end{document}