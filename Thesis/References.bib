@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@book{bellman1957dynamic,
  title={Dynamic programming},
  author={Bellman, Richard},
  year={1957},
  publisher={Princeton university press}
}

@inproceedings{cassandra1994acting,
  title={Acting optimally in partially observable stochastic domains},
  author={Cassandra, Anthony R and Kaelbling, Leslie Pack and Littman, Michael L},
  booktitle={Proceedings of the Twelfth National Conference on Artificial Intelligence},
  pages={1023--1028},
  year={1994},
  volume={2}
}

@inproceedings{auer2002finite,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicol{\`o} and Fischer, Paul},
  booktitle={Machine learning},
  volume={47},
  pages={235--256},
  year={2002},
  organization={Springer}
}

@article{strehl2008analysis,
  title={An analysis of model-based interval estimation for Markov decision processes},
  author={Strehl, Alexander L and Littman, Michael L},
  journal={Journal of Computer and System Sciences},
  volume={74},
  number={8},
  pages={1309--1331},
  year={2008},
  publisher={Elsevier}
}


% For Temporal Difference (TD) Learning
@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  pages={9--44},
  year={1988},
  publisher={Springer}
}

% For Sarsa
@techreport{rummery1994line,
  title={On-line Q-learning using connectionist systems},
  author={Rummery, Gavin A and Niranjan, Mahesan},
  year={1994},
  institution={Cambridge University Engineering Department}
}

% For Q-Learning
@phdthesis{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  school={University of Cambridge}
}

% For Deep Q-Learning (DQN) and Target Networks
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

% For Experience Replay
@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  pages={293--321},
  year={1992},
  publisher={Springer}
}

% For general MARL overview
@article{busoniu2008comprehensive,
  title={A comprehensive survey of multi-agent reinforcement learning},
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={38},
  number={2},
  pages={156--172},
  year={2008},
  publisher={IEEE}
}

% For Dec-POMDPs
@book{oliehoek2016concise,
  title={A concise introduction to decentralized POMDPs},
  author={Oliehoek, Frans A and Amato, Christopher},
  year={2016},
  publisher={Springer}
}

% For Learning Paradigms in MARL (CTDE)
@article{oroojlooy2023review,
  title={A review of cooperative multi-agent deep reinforcement learning},
  author={Oroojlooy, Afshin and Hajinezhad, Davood},
  journal={Applied Intelligence},
  volume={53},
  number={10},
  pages={12907--12965},
  year={2023},
  publisher={Springer}
}

% For Value Decomposition Networks (VDN)
@inproceedings{sunehag2017value,
  title={Value-decomposition networks for cooperative multi-agent learning},
  author={Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech M and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z and Tuyls, Karl and others},
  booktitle={Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={2085--2087},
  year={2018}
}

% For QMIX
@inproceedings{rashid2018qmix,
  title={Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning},
  author={Rashid, Tabish and Samvelyan, Mikayel and Schroeder, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  booktitle={International conference on machine learning},
  pages={4295--4304},
  year={2018},
  organization={PMLR}
}

% For Hypernetworks
@article{ha2016hypernetworks,
  title={Hypernetworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V},
  journal={arXiv preprint arXiv:1609.09106},
  year={2016}
}