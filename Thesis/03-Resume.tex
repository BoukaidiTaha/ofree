

\mychapter{0}{Abstract}

Cooperative Multi-Agent Reinforcement Learning (MARL) is a powerful paradigm for solving complex coordination tasks, but its effectiveness is often limited by the challenge of partial observability, where agents must act based on incomplete local information. While methods like the Multi-Agent Masked Auto-Encoder (${MA}^2E$) attempt to address this by learning to reconstruct global information, their reliance on a  random masking strategy creates a disconnect between the representation learning and the primary reinforcement learning objective.

This thesis introduces a novel framework, Learning-Informed Masking for Multi-Agent Reinforcement Learning ($LI-{MA}^2E$), which bridges this gap. Our approach introduces a \textit{Policy-Aware Scorer} that leverages real-time training signals, such as the Temporal Difference (TD) error, to intelligently and adaptively mask agents that are performing poorly. This forces the auto-encoder to focus its representational power where it is needed most, creating a more efficient and effective learning curriculum.

Through a rigorous empirical evaluation on the StarCraft Multi-Agent Challenge (SMAC) benchmark, we demonstrate that this learning-informed approach is superior to random masking. Our methodical investigation shows that a simple, adaptive masking strategy based on the mean TD-error yields the best performance. The final $LI-{MA}^2E$ framework not only achieves faster task completion (win rate) but also learns higher-quality and more robust policies (mean test return), which is explained by a more stable underlying learning process. This work confirms that dynamically aligning the self-supervised reconstruction task with the RL objective is a powerful mechanism for enhancing MARL performance under partial observability.



\vspace{1cm}



\noindent\rule[2pt]{\textwidth}{0.5pt}

{\textbf{Keywords :}}
Multi-Agent Reinforcement Learning (MARL), Partial Observability, Masked Auto-Encoder, Representation Learning, Deep Reinforcement Learning, Coordination.
\\
\noindent\rule[2pt]{\textwidth}{0.5pt}
\clearpage



\mychapter{0}{Résumé}



L'apprentissage par renforcement multi-agents (MARL) est un paradigme puissant pour résoudre des tâches de coordination complexes, mais son efficacité est souvent limitée par le défi de l'observabilité partielle, où les agents doivent agir sur la base d'informations locales incomplètes. Bien que des méthodes telles que le Multi-Agent Masked Auto-Encoder (${MA}^2E$) tentent de résoudre ce problème en apprenant à reconstruire une information globale, leur dépendance à une stratégie de masquage aléatoire et agnostique de la politique crée une déconnexion entre l'apprentissage de la représentation et l'objectif principal de l'apprentissage par renforcement.

Cette thèse introduit un nouveau framework, le Masquage Informé par l'Apprentissage pour l'Apprentissage par Renforcement Multi-Agents ($LI-{MA}^2E$), qui comble cette lacune. Notre approche introduit un \textit{Évaluateur Conscient de la Politique} qui utilise des signaux d'entraînement en temps réel, tels que l'erreur de différence temporelle (TD-error), pour masquer intelligemment et de manière adaptative les agents peu performants. Cela contraint l'auto-encodeur à concentrer sa puissance de représentation là où elle est le plus nécessaire, créant ainsi un curriculum d'apprentissage plus efficace.

À travers une évaluation empirique rigoureuse sur le benchmark StarCraft Multi-Agent Challenge (SMAC), nous démontrons que cette approche informée par l'apprentissage est supérieure au masquage aléatoire. Notre investigation méthodique montre qu'une stratégie de masquage simple et adaptative basée sur l'erreur TD moyenne donne les meilleures performances. Le framework final $LI-{MA}^2E$ permet non seulement d'accomplir les tâches plus rapidement (taux de victoire), mais aussi d'apprendre des politiques de meilleure qualité et plus robustes (retour moyen), ce qui s'explique par un processus d'apprentissage sous-jacent plus stable. Ce travail confirme que l'alignement dynamique de la tâche de reconstruction auto-supervisée avec l'objectif de l'AR est un mécanisme puissant pour améliorer les performances du MARL en condition d'observabilité partielle.

\vspace{1cm}


\noindent\rule[2pt]{\textwidth}{0.5pt}

{\textbf{Mots clés :}}
Apprentissage par Renforcement Multi-Agents (MARL), Observabilité Partielle, Auto-Encodeur Masqué, Apprentissage de Représentation, Apprentissage par Renforcement Profond, Coordination.

